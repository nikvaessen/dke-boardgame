\documentclass{ba-kecs}
\usepackage{graphicx}
\usepackage[]{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}




\title{AI Implementations for Hex}
\runningtitle{AI for Hex}

\author{A. Steckelberg, N. Vaessen, J.L. Velasquez, J. Vermazeren, T. Wall, X. Weber}

\begin{document}

\maketitle

\begin{abstract}
 Hex is a classic board game invented in 1942 by Piet Hein and independently by John Nash in 1948. In this paper there is research into alpha-beta and Monte Carlo Tree Search Hex players, as well as comparasions of different evaluation functions which includes Dijkstra, Electric Circuit. In addition to this Monte Carlo Tree Search specific heuristics are compared.   

 \textbf{GIVE RESULTS!!!!}
\end{abstract}

\section{Introduction}
Hex is a well-known board game first created in 1942 by Piet Hein, a Danish physicist, with later improvements being made by John F. Nash in 1948. It gained the name Hex in 1952 when a version of the game was released by the firm Parker Brothers, Inc \cite{gardener1959hex}. The basic idea of Hex is to create a bridge across a diamond shape board of hexagons. The generally accepted normal size of the board is 11 $\times$ 11, however it can be played on boards of differing sizes. One player will try to create a bridge from the top of the board to the bottom, while the other player tries to make one from left to right. The game is fully deterministic, as by finishing one player’s bridge will always block the other player. As the first player is always at an advantage, Hex has a unique rule to address this. Called the Pie Rule, it states that after the second player has had their first move, they may switch the placement of the first two tiles.

The history of creating algorithms to play Hex is rich with successful attempts, all using a different approach. In this paper, several of these attempts will be implemented and tested against both each other, while also seeing if limiting factors in these algorithms (time per move, tree depth, etc.) have major effects on the results.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{figure_hexboard.png}
	\caption{An example of a hexboard}
	\label{fig:hexboard}
\end{figure}

\section{Rules of Hex}
The rules of Hex are simple. Each player is given a colour, typically blue and red or black and white. The players then take turns placing tiles down on the board to claim a space. The aim is to create a path from one side of the board to the other, either top to bottom or left to right. Since the first player is always at distinct advantage there exists the Pie Rule, which states that after the second player has made their first move they may swap the positions of the first two tiles. This rule is not implemented in the algorithms described in this paper, as both algorithms will have equal time as player one and player two.

\textbf{BACKGROUND JOHN NASH STEALING ARGUMENT SWAP RULE} \cite{nash1952}

\section{Algorithms}
\subsection{Game tree}
Game trees closely follows a \textit{Markov Decision Process} (MDP) in which every node of the tree is represented by a State $s \in S$ and the connection between the nodes is represented by an action $a \in A$. \cite{browne2012survey}
\begin{itemize}
	\item $S$: a set of all possible states of the board
	\item $A$: a set of all possible actions that can transition from one state to the other
	\item $P_a(s, s')$: the probability of reaching state $s'$ by using action $a$ on state $s$
	\item $R_a(s, s')$: the reward for reaching state $s'$ by using action $a$ on state $s$
\end{itemize}
\subsection{Alpha-beta}
MiniMax search with $\alpha$ - $\beta$ pruning is a standard approach to deterministic games with perfect information and Hex is such an environment. MiniMax search investigates future positions of the board and evaluates them. It was used by the first Hex playing machine by Shannon and Moore \cite{shannon1953computers} and alpha-beta based Hex players have dominated Hex competitions until 2008.\cite{arneson2010monte} 

MiniMax search determines what the best move in each position is using a game tree. MiniMax evaluates the game tree using an evaluation function from the perspective of one player, referred to as MAX. The opponent is referred to as MIN. \cite{russell1995modern} MiniMax is a depth-first search. The game tree is evaluated from the leaf towards the root. The root of the tree is associated with MAX. Each level of the tree is associated with a player in an alternating way. Nodes with an odd depth are associated with MIN and nodes with an even depth are associated with MAX. 

MAX nodes receive the maximal evaluation of the nodes one level below it as MAX aims to maximize utility. MIN nodes receive the minimal evaluation of the nodes one level below it as MIN aims to minimize the utility of MAX. \cite{russell1995modern}

%\begin{algorithm}
%	\caption{Minimax}
%	\label{alg:minimax}
%	\begin{algorithmic}
%		\Function{Minimax-Decision}{$game$}{\textbf{ returns $an$ $operator$}}
%		\For{\textbf{each} $op$ \textbf{in} OPERATORS[$game$]}
%		\State VALUE[$OP$] $\leftarrow$ 
%		\EndFor\label{}\\
%		\Return{the $op$ with the highest VALUE[$op$]}
%		\EndFunction \\
%	\end{algorithmic}
%	\begin{algorithmic}
%		\Function{Minimax-Value}{$state$,$game$}{\textbf{ returns $a$ $utility$ $value$}}
%		\If{TERMINAL-TEST[$game$]($state$)} 
%		\State	\Return {UTILITY[$game$]($state$)}
%		\ElsIf{MAX is to move in $state$}
%		\State	\Return {the highest MINIMAX-VALUE \State of SUCCESSORS($state$)}
%		\Else
%		\State 	\Return {the lowest MINIMAX-VALUE \State of SUCCESSORS($state$)}
%		\EndIf
%		\EndFunction \\
%	\end{algorithmic}
%\end{algorithm}

$\alpha$ - $\beta$ pruning is a strategy to simplify the search. The idea behind alpha-beta pruning is to prune subtrees for which we know that they do not yield a better strategy. ''Consider a node n somewhere in the tree (...)  such that Player has a choice of moving to that node. If Player has a better choice m either at the parent node of n, or at any choice point further up, then n will never be reached in actual play. So, once we have found out enough about n (by examining some of its descendants) to reach this conclusion, we can prune it.'' \cite{russell1995modern}. This strategy increases the efficiency of the search. Pseudocode for the Minimax algorithm with alpha-beta pruning is provided below. (see Algorithm \ref{alg:alphabeta})

\begin{algorithm}
	\caption{Alpha-beta}
	\label{alg:alphabeta}
	\begin{algorithmic}
		\Function{Max-Value}{$stategame,\alpha, \beta$}{\textbf{ returns } the minimax value of $state$}
		\State \textbf{inputs: } $state$, current state in game
		\State $game$ , game description
		\State $\alpha$, the best score of MAX along the path to $state$
		\State $\beta$, the best score for MIN along the path to $state$
		\State 
		\If{CUTOFF-TEST($state$)}
			\State \Return{EVAL($state$)}
		\EndIf
		\For{\textbf{each} $s$ \textbf{in} SUCCESORS($state$)}
			\State $\alpha$ $\leftarrow$ MAX($\alpha$, MIN-VALUE($s$, $game$, $\alpha$, $\beta$))
		\EndFor \label{}
		\EndFunction\\
		
		\Function{Min-Value}{$state, game, \alpha, ft$}{\textbf{ returns } the minimax value of $state$} \\
		\If{CUTOFF-TEST($state$)}
		\State \Return{EVAL($state$)}
		\EndIf
		\For{\textbf{each} $s$ \textbf{in} SUCCESSORS($state$)}
			\State $ft$ $\leftarrow$ MIN($ft$, MAX-VALUE($s, game , \alpha, \beta$)) 
			\If{$ft$ \textless $a$}
		 \Return{$a$}
			\EndIf
		\EndFor \label{}
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

\subsection{Evaluation Functions}
Shannon and Moore \cite{shannon1953computers} model the board as an electrical circuit which overcomes this limitation as it considers all paths in parallel. Their ideas have been used by Anshelevich \cite{anshelevich2002hierarchical} to develop the Hex player Hexy. The board is represented as a graph. Positions on the board are vertices and adjacent positions are connected by edges. Edges of the graph are resistors and resistance is assigned in the following way: 
\\
\\
For Black's circuit:  \\
\begin{equation*}
r_B(c)=\begin{cases}
1, & \text{if $c$ is empty},\\
0, & \text{if $c$ is occupied by a black piece},\\
+\infty & \text{if $c$ is occupied by a white piece}.
\end{cases}
\end{equation*}
\\
And likewise, for the circuit of the other player . 
One can see that positions occupied by the opponent are not a part of the circuit as they have an infinite resistance. A voltage is then applied to the two sides that the player needs to connect. \cite{anshelevich2002hierarchical}.

By calculating the equivalent resistance of the board an evaluation of the board is achieved. The lower the resistance, the better the evaluation. The equivalent resistance can be calculated using a system of linear equations based on Kirchhoff’s Current Law. \cite{anshelevich2002hierarchical}

H. Challup, Mellor and Rosamund \cite{chalup2005machine} also used Dijkstra’s algorithm for shortest path finding in a graph. In contrast to the previous evaluation this evaluation only considers a single path and not the whole board.  The board is represented as a graph. Positions on the board are vertices and adjacent positions are connected by edges. The edges are weighted using the following evaluation. (\ref{dijkstra}) Positions occupied by the opponent do not belong to the graph.
\begin{center}
\begin{equation}
\label{dijkstra}
\tag{1. Dijkstra evaluation \cite{chalup2005machine}}
W(p_1,p_2)=\begin{cases}
0, & \text{if $p_1$  and $p_2$ are both occupied}\\
   & \text{by the player},\\
1, & \text{if one out of $p_1$ and $p_2$ is}\\
   & \text{occupied by the player},\\
2 & \text{if both positions are unoccupied}.
\end{cases}
\end{equation}
\end{center}
The shortest path between the two sides of the board that need to be connected by the player is calculated. Shorter paths are preferred by the player. 

\subsection{Monte Carlo Tree Search}
A powerful AI technique for board games is the Monte Carlo Tree Search(MCTS). It applies optimization techniques of random simulations to the game tree. 



For Hex, the probability of reaching a state for any action is 1 as long as it is a legal move. 
\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{figure_mcts.jpg}
	\caption{The four steps of Monte-Carlo Tree Search \cite{chaslot2008progressive}}
	\label{fig:mcts}
	\end{figure}
The MCTS algorithm can be divided into four main parts (see figure \ref{fig:mcts}): 
\begin{enumerate}
\item Selection: In this part the algorithm looks for the most urgent node with a state $s$ that has not being visited before.
\item Expansion: one or more nodes are expanded by adding leaf nodes according the values left in $A$.
\item Simulation: a simulation takes place in order to assign a reward value to the newly added node/s.
\item Backpropagation: the statistics of the tree are updated through backpropagation.
\end{enumerate}
These four simple parts of the algorithm can be divided into two policies that simplify the process.
\begin{itemize}
\item \textit{Tree Policy}: in this policy the stages of selection and expansion take place. The random nodes are selected and expanded, ready to receive their reward value.
\item \textit{Default Policy}: in this policy the simulation takes place and a reward value is given to the node 
\end{itemize}
The backpropagation stage of the MCTS does not occur within any policy as it just represents the update of the tree after the reward, which is used in later cycles for the tree policy to select and expand new nodes.

\begin{algorithm}
    \caption{Monte-Carlo Tree Search}
    \label{alg:MCTS}
    \begin{algorithmic}
        \Procedure{MCTS}{$s_0$} 
            \State create root node $v_0$ with state $s_0$
            \While{within computational budget} 
                \State $v_l \gets$ TreePolicy($v_0$)
                \State $ \Delta \gets$ DefaultPolicy($s(v_l)$)
                \State Backup($v_l, \Delta$)
            \EndWhile\label{}
            \State \textbf{return} $a$(BestChild($v_0$))
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
In Algorithm \ref{alg:MCTS} we see the general structure of the MCTS. The root of the tree will be the board state at the beginning current turn. After the tree has fully expanded according to computational budget using the repeated MCTS process explained before, the best child is selected. We return the action $a$ which connects the current state $s$ to the best child.
\subsection{Improvements on General MCTS}
Different heuristics can be used on top the MCTS algorithm to enhance its performance.
\subsubsection{Upper Confidence Bounds for Trees (UCT)}

The UCT is an algorithm which adapts the Upper Confidence Bounds (UCB) exploitation and exploration of trade-offs as a tree policy. 
Selecting a child node to traverse in the selection part of MCTS can be seen as a multi-armed bandit problem.\cite{auer2002using}

The UCT tree policy works by choosing the node $v$ that maximizes the independent multi-armed bandit problem:
\begin{center}
\begin{equation*}
	UCT = \bar{X_v} + C_p \sqrt{\frac{\ln n}{n_v}}
\end{equation*}
\end{center}
In the equation above $n$ represents the amount of time the current node has being visited, $n_v$ the amount of time a  child node has been visited, and the constant $C_p \geq0 $ can be used to weight the importance of exploration. If a child is not visited ($n_v = 0$) the exploration term is set to $\infty$.

\textbf{XXXX NEEDS TEXT ABOUT ALGORITHMS}

\begin{algorithm}
	\caption{Upper Confidence Threshold}
    \label{UCT}
    \begin{algorithmic}
    	\Function{UCTSearch}{$s_0$}
        	\State $v_0 \leftarrow$ create root node from $s_0$
            \While{within computational budget}
            	\State $v_l \leftarrow$ TreePolicy($v_0$)
                \State $\Delta \leftarrow$ DefaultPolicy($s(v_l)$)
                \State Backup($v_0, \Delta$)
            \EndWhile
            \State \textbf{return} $a$(BestChild($v_0,0$))
        \EndFunction
        \State
        \Function{TreePolicy}{$v$}
        	\While{$v$ is non-terminal}
            	\If{$v$ not fully expanded}
                	\State \textbf{return} Expand($v$)
                \Else
                	\State $v \leftarrow$ BestChild($v_0,Cp$)
                \EndIf
            \EndWhile
           	\State \textbf{return} $v$
        \EndFunction
        \State
        \Function{Expand}{$v$}
        	\State $a\leftarrow$ any random unused action in $A(s(v))$
            \State add new child $v'$ to $v$ where:
            \State $s(v') =$ Result($s(v),a$)
            \State $a = a(v')$
            \State \textbf{return} $v'$
        \EndFunction
        \State
        \Function{BestChild}{$v,c$}
        	\State \textbf{return} $\operatornamewithlimits{arg max}\limits_{v' \in children(v)}\frac{Q(v'))}{N(v')} + c\sqrt{\frac{\ln N(v)}{N(v')}}
$
        \EndFunction
        \State
        \Function{DefaultPolicy}{$s$}
        	\While{$s$ is non-terminal}
            	\State choose $a \in A(s)$ uniformly at random
                \State $s \leftarrow$ Result($s, a$)
                \State \textbf{return} reward for state $a$
            \EndWhile
        \EndFunction
        \State
        \Function{Backup}{$v, \Delta$}
        	\While{$v$ is not null}
            	\State $N(v) \leftarrow N(v)+1$
                \State $Q(v) \leftarrow Q(v)+\Delta(v,p)$
                \State $v \leftarrow$ parent of $v$
            \EndWhile
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{All Moves As First (AMAF)}
Originally the AMAF method has been introduced to improve the learning process inside MCTS trees. The selection of the UCT algorithm is based on a estimated value obtained by simulating the move in the node a couple times. This can lead to a problem if there is a large state space since the algorithm has to do many simulations before it can sample all the moves in a node. To conquer this issue AMAF, also known as RAVE, is introduced.
For every node the algorithm stores for all legal moves the following values 
\begin{itemize}
	\item The average UCT result obtained from all simulations in which move $a$ is performed in state $s$
	\item The average AMAF result, obtained from all the simulations in which move $a$ is performed further down the path that passes by node $s$
\end{itemize}
When backpropagating the result of a simulation in a certain node \textit{t} in the tree, the UCT result is updated for the move $a$ that was directly played in the state, and the AMAF value is updated for all the legal moves in node $t$ that have been encountered at a later stage in the simulations. AMAF will encounter more samples and will use them to make better predictions. However, the disadvantage of this information is that AMAF information is more global whilst the UCT information is more local. This makes AMAF scores useful for less visited nodes, but when the number of visits increases, the UCT values should become more important. \cite{sironicomparison}

To solve this issue the AMAF algorithm keeps track of the two scores separately and uses a weight $\beta$ to reduce the importance of the AMAF score over time. 

\begin{center}
	\begin{equation*}
    \beta = \frac{\tilde{n}}{n+\tilde{n} + 4n\tilde{n}\tilde{b}^2} 	\cite{gelly2011monte}
	\end{equation*}
\end{center}

To calculate the $\beta$ value AMAF uses the equation denoted above. Where $\tilde{n}$ is the visits of AMAF and $n$ is total number of visits or amount of simulations. The equation also includes a bias $\tilde{b}$ which is in this algorithm a parameter which can be adjusted by testing the performance. \cite{gelly2011monte}

\begin{algorithm}
	\caption{All Moves As First (AMAF)}
	\label{alg:AMAF}
	\begin{algorithmic}
		\Function{backpropagate}{$node$, $reward$, $times$} 
		\State innerBackpropagate($node$, $reward$, $times$)
		\For{every child of the parent of $node$}
		\State amafForwardPropagation($child$, $reward$, $times$, $action$)
		\EndFor\label{}
		\EndFunction \\
		
		\Function{innerBackpropagate}{$node$, $reward$, $times$}
		\State $visits$ $\leftarrow$ $visits + 1$
		\State $reward$ $\leftarrow$ $reward + 1$
		\If{$node \neq root$}
			\State innerBackpropagate($parent$, $reward$, $times$)
		\EndIf
		\EndFunction \\
		
		\Function{amafFowardPropagation}{$node$, $reward$, $times$, $action$}
		\If{$node action$ $=$ $action$}
			\State $visits$ $\leftarrow$ $visits + 1$
			\State $reward$ $\leftarrow$ $reward + 1$
		\Else
			\For{every child of the parent of the $node$}
				\State amafForwardPropagation($child$, $reward$, $times$, $action$)
			\EndFor
		\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}
To understand AMAF there is pseudocode above in algorithm \ref{alg:AMAF}. In comparison with normal back propagation it also updates the AMAF values for certain actions.


\subsubsection{Parallelization}
There are three different types of parallelisation, depending on in what stage Monto-Carlo Tree Search is parallelised. The three options are: leaf parallelization, root parallelization and tree parallelization. 

In this paper only  leaf parallelization was considered since this is a straightforward to implement. To select a leaf node this method uses one main thread, which is the same thread that runs the game logic. From this leaf node it simulates independent games for each available thread. When all games are finished the main thread backpropagates all the values.\cite{chaslot2008parallel} Leaf parallelization is depicted in figure \ref{fig:leaf}.

A problem with leaf parallelization is that threads have to wait for each other which could slow then the process a way to solve this is end simulations for a move if a certain amount of games is won. For instance, if 16 threads are available, and 8 (faster) finished games are all losses, it will be highly probable that most games
will lead to a loss. Therefore, playing 8 more games is a waste of computational power. This will enable the program to traverse the tree more often.\cite{chaslot2008parallel}

To reduce the cost of using threads, both the \textit{Java ExecutorService} interface and \textit{ThreadGroups} were implemented and compared.
\begin{figure}
	\centering
	\includegraphics[]{figure_leafparallization}
		\caption{Visual explanation of leaf paralellization, the down arrows are the threads \cite{chaslot2008parallel}}
		\label{fig:leaf}
\end{figure}


\subsubsection{Tree reuse}
The ordinary implementation of Monte-Carlo Tree Search builds a new tree for every move a player has to make. An obvious improvement to speed up the algorithm would be that the program does not build a tree every move but it tries to keep the tree it made in the previous move by searching through the children of the previous move and check if the node already exist. Since the program does not know the move of the opponent it could be that the move does not exist This strategy enables the algorithm to have already some reward values for certain moves so the predictions can be done more accurately.

\section{Experiments}
All experiments were run on the following machines:
\begin{itemize}
	\item Machine 1, Intel i5 2600 2.4GHz, 8GB ram
	\item Machine 2, Intel i7 4720HQ 2.6GHz, 8GB ram
		\item Machine 3, Intel i5 5257U 2.7GHz, 8GB ram
	\item Machine 4, Intel i5 3210M 2.5GHz, 6GB ram
\end{itemize}
Every test was run 20 times, with each algorithm being player one 10 times. For the first experiment, simulations per iteration (SPI) and the exploration coefficient (C) were changed to find the best combination of the two. These results will be used when testing the MCTS evaluation function comparison and MCTS parallelization tests. 

In the second test, the different evaluation functions for MCTS were compared to see which one is superior. This will be used when MCTS is compared against the Alpha-Beta algorithm. 

For the third experiment, the use of leaf parallelization in MCTS was tested to see if this yielded any performance or efficiently improvements. If it does, this will also be included when MCTS is compared against Alpha-Beta. The fourth experiment compared the evaluation functions for the Alpha-Beta algorithm. Whichever was more successful will be used when comparing against MCTS. (Machine 4) Finally, the last experiment was to do a direct comparison of Alpha-Beta and MCTS with their most optimal settings to determine which algorithm is the best.

Every test was run 20 times, with each algorithm being player one 10 times. 

\textbf{ XXXX Some settings stuff, fun times}
\section{Results}
Explanation of results of the experiments
\section{Conclusion}
Conclusion on the explanation of the result of the experiments
\section{Future Research}
One of the methods that was initial looked into was Q-Learning, which is a type of 
\bibliographystyle{apacite}
\bibliography{biblio}



\end{document}

